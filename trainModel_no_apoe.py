import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.utils import class_weight
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import os
from tensorflow.keras.regularizers import l2

# =========================================================================
# 1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°/í†µí•©
# =========================================================================
print("=" * 70)
print("SCD ì œì™¸, APOE ì—†ëŠ” ì¹˜ë§¤ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
print("=" * 70)
print("\n--- 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° í†µí•© ì‹œì‘ ---")

# ì€ì„œë‹˜ì˜ ì‹¤ì œ ê²½ë¡œ
BASE_PATH = "C:/Users/ì€ì„œ/Desktop/Univ/cohort/soul/soul/"

try:
    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    df_clinical = pd.read_excel(os.path.join(BASE_PATH, "screening_data_1001.xlsx"))
    df_snsb = pd.read_excel(os.path.join(BASE_PATH, "SNSB_1000.xlsx"))

    # ID ì»¬ëŸ¼ ì´ë¦„ í†µì¼
    if 'Subject ID' in df_snsb.columns:
        df_snsb.rename(columns={'Subject ID': 'SubjectID'}, inplace=True)

    # ë°ì´í„° í†µí•© (Inner Join)
    df_final_tabular = pd.merge(df_clinical, df_snsb, on='SubjectID', how='inner')
    df_dnn = df_final_tabular.copy()

    print(f"âœ… ë°ì´í„° í†µí•© ì™„ë£Œ. ìµœì¢… ë°ì´í„° í¬ê¸°: {df_dnn.shape[0]} í–‰\n")

except FileNotFoundError:
    print(f"ì˜¤ë¥˜: ì§€ì •ëœ ê²½ë¡œ({BASE_PATH})ì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì‹­ì‹œì˜¤.")
    exit()
except Exception as e:
    print(f"ë°ì´í„° ë¡œë“œ ë˜ëŠ” í†µí•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit()

# =========================================================================
# 2. ì „ì²˜ë¦¬: ë³€ìˆ˜ ì„ íƒ, ì¸ì½”ë”©, ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# =========================================================================
print("--- 2. ì „ì²˜ë¦¬ (ë³€ìˆ˜ ì¸ì½”ë”© ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬) ì‹œì‘ ---")

# 2.1. í•µì‹¬ ë³€ìˆ˜ ì„ íƒ (APOE ì œì™¸)
core_features = [
    'SubjectID', 'dm_10', 'dm_11', 'DIA_01', 'dm_06', 
    'K_MMSE_total_score', 'kiadl_total', 'Digit_span_Forward', 
    'SVLT_recall_total_score', 'RCFT_immediate_recall'
]
df_dnn = df_dnn[core_features].copy()

# 2.2. ëª©í‘œ ë³€ìˆ˜(Target) ì¸ì½”ë”© - âš ï¸ SCD ì œì™¸!
# CN: 0, MCI: 1, Dementia: 2
diagnosis_map = {
    'CN': 0, 
    # 'SCD': 0,  # â† SCD ì œê±°!
    'MCI': 1, 
    'Dem': 2,        # â† ì‹¤ì œ ë°ì´í„°ì˜ Dementia í‘œê¸°
    'Dementia': 2, 
    'AD': 2, 
    'VD': 2, 
    'OTHERS': 2
}
df_dnn['Target'] = df_dnn['DIA_01'].map(diagnosis_map)

print(f"ë§¤í•‘ ì „ ë°ì´í„° í¬ê¸°: {df_dnn.shape[0]} í–‰")
print(f"ì§„ë‹¨ ë¶„í¬:\n{df_dnn['DIA_01'].value_counts()}")
print(f"\nTarget ë§¤í•‘ í›„ ê²°ì¸¡ì¹˜ ê°œìˆ˜: {df_dnn['Target'].isna().sum()}ê°œ (SCD ë° ê¸°íƒ€)")

df_dnn.drop(columns=['DIA_01'], inplace=True)
df_dnn.dropna(subset=['Target'], inplace=True) 

print(f"\nSCD ì œê±° í›„ ë°ì´í„° í¬ê¸°: {df_dnn.shape[0]} í–‰")
print(f"Target ë¶„í¬:\n{df_dnn['Target'].value_counts()}\n")

# 2.3. ì„±ë³„ ì¸ì½”ë”©
df_dnn['Sex_Female_1'] = df_dnn['dm_06'].replace({1: 0, 2: 1})
df_dnn.drop(columns=['dm_06'], inplace=True)

# 2.4. ìˆ˜ì¹˜í˜• í”¼ì³ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (í‰ê·  ëŒ€ì²´)
numerical_cols_for_imputation = [
    'dm_10', 'dm_11', 'K_MMSE_total_score', 'kiadl_total', 
    'Digit_span_Forward', 'SVLT_recall_total_score', 'RCFT_immediate_recall'
]
for col in numerical_cols_for_imputation:
    df_dnn[col].fillna(df_dnn[col].mean(), inplace=True)

# =========================================================================
# 3. í‘œì¤€í™” ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™” (KeyError í•´ê²°)
# =========================================================================
print("--- 3. í‘œì¤€í™” ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì‹œì‘ ---")

numerical_cols = numerical_cols_for_imputation 
scaler = StandardScaler()
df_dnn[numerical_cols] = scaler.fit_transform(df_dnn[numerical_cols])

# ğŸ’¥ FIX: KeyError ë°©ì§€ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
df_dnn.reset_index(drop=True, inplace=True) 
print("âœ… ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ.\n")

# =========================================================================
# 4. ì¸µí™”ì¶”ì¶œ ê¸°ë°˜ ë°ì´í„° ë¶„í•  (Split)
# =========================================================================
print("--- 4. ì¸µí™”ì¶”ì¶œ ê¸°ë°˜ ë°ì´í„° ë¶„í•  ì‹œì‘ ---")

X = df_dnn.drop(columns=['SubjectID', 'Target']).copy()
y = df_dnn['Target']

# Train/Validation (80%) ì™€ Test (20%) ë¶„í• 
sss_test = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_val_index, test_index in sss_test.split(X, y):
    df_test = df_dnn.loc[test_index].copy()
    df_test['Data_Set'] = 'Test'
    X_train_val = X.iloc[train_val_index]
    y_train_val = y.iloc[train_val_index]

# Train (64%) ì™€ Validation (16%) ë¶„í• 
sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, val_index in sss_val.split(X_train_val, y_train_val):
    df_train = df_dnn.loc[train_index].copy()
    df_validation = df_dnn.loc[val_index].copy()
    
    df_train['Data_Set'] = 'Train'
    df_validation['Data_Set'] = 'Validation'
    
    # ëª¨ë¸ í•™ìŠµìš© X, y ì •ì˜
    X_train = df_train.drop(columns=['SubjectID', 'Target', 'Data_Set'])
    y_train = df_train['Target']
    X_val = df_validation.drop(columns=['SubjectID', 'Target', 'Data_Set'])
    y_val = df_validation['Target']
    X_test = df_test.drop(columns=['SubjectID', 'Target', 'Data_Set'])
    y_test = df_test['Target']

print(f"í›ˆë ¨ ì„¸íŠ¸: {df_train.shape[0]}, ê²€ì¦ ì„¸íŠ¸: {df_validation.shape[0]}, í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {df_test.shape[0]}")
print(f"\nì…ë ¥ í”¼ì²˜ ê°œìˆ˜: {X_train.shape[1]}")
print(f"í”¼ì²˜ ëª©ë¡: {list(X_train.columns)}\n")

# =========================================================================
# 5. ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥
# =========================================================================
print("--- 5. ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥ ì‹œì‘ ---")

df_train.to_csv('dnn_tabular_train_data_no_scd.csv', index=False, encoding='utf-8')
df_validation.to_csv('dnn_tabular_validation_data_no_scd.csv', index=False, encoding='utf-8')
df_test.to_csv('dnn_tabular_test_data_no_scd.csv', index=False, encoding='utf-8')
df_master = pd.concat([df_train, df_validation, df_test], axis=0)
df_master.to_csv('dnn_tabular_master_data_no_scd.csv', index=False, encoding='utf-8')

print("âœ… CSV íŒŒì¼ 4ê°œ ì €ì¥ ì™„ë£Œ.\n")

# =========================================================================
# 6. DNN ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ ì¤€ë¹„
# =========================================================================
print("--- 6. DNN ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ ì¤€ë¹„ ---")

# 6.1. í´ë˜ìŠ¤ ë° í”¼ì²˜ ê°œìˆ˜ ì •ì˜
INPUT_FEATURES = X_train.shape[1] 
NUM_CLASSES = len(y_train.unique()) 

print(f"ì…ë ¥ í”¼ì²˜ ìˆ˜: {INPUT_FEATURES}")
print(f"í´ë˜ìŠ¤ ìˆ˜: {NUM_CLASSES}")
print(f"í´ë˜ìŠ¤ ë ˆì´ë¸”: {sorted(y_train.unique())}\n")

# 6.2. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ë°°ì—´ ìƒì„±
class_weights = class_weight.compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train.to_numpy()
)
class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}
sample_weights = np.array([class_weight_dict[label] for label in y_train])

print(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weight_dict}\n")

# 6.3. DNN ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜
def build_tabular_dnn(input_shape, feature_vector_dim=64, num_classes=3):
    input_layer = Input(shape=(input_shape,), name='tabular_input')
    
    x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(input_layer)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    # íŠ¹ì§• ì¶”ì¶œì¸µ
    feature_vector = Dense(feature_vector_dim, activation='relu', name='tabular_feature_vector')(x)
    
    # ìµœì¢… ë¶„ë¥˜ ì¶œë ¥ì¸µ
    classifier_output = Dense(num_classes, activation='softmax', name='tabular_classification_output')(feature_vector)
    
    model = Model(inputs=input_layer, outputs=[feature_vector, classifier_output])
    return model

# ëª¨ë¸ ìƒì„± ë° ì»´íŒŒì¼
dnn_model = build_tabular_dnn(input_shape=INPUT_FEATURES, num_classes=NUM_CLASSES)

dnn_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
    loss={
        'tabular_feature_vector': None,  
        'tabular_classification_output': 'sparse_categorical_crossentropy'
    },
    metrics={'tabular_classification_output': 'accuracy'}
)

print("--- DNN ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ ---")
print(dnn_model.summary())

# =========================================================================
# 7. ëª¨ë¸ í•™ìŠµ (Training)
# =========================================================================
print("\n" + "=" * 70)
print("--- 7. ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---")
print("=" * 70 + "\n")

early_stopping = EarlyStopping(
    monitor='val_tabular_classification_output_loss', 
    patience=20, 
    restore_best_weights=True,
    mode='min',
    verbose=1
)

checkpoint_filepath = 'best_dnn_tabular_model_no_scd.h5'
model_checkpoint = ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_tabular_classification_output_loss',
    save_best_only=True,
    verbose=1,
    mode='min'
)

# ëª¨ë¸ í•™ìŠµ
history = dnn_model.fit(
    X_train, 
    (None, y_train),
    
    validation_data=(
        X_val, 
        (None, y_val) 
    ),
    
    epochs=150, 
    batch_size=32,
    callbacks=[early_stopping, model_checkpoint],
    
    sample_weight=(None, sample_weights), 
    
    verbose=1
)

print(f"\nâœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. ìµœì  ëª¨ë¸ì€ '{checkpoint_filepath}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# =========================================================================
# 8. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€
# =========================================================================
print("\n" + "=" * 70)
print("--- 8. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€ ---")
print("=" * 70 + "\n")

try:
    # ìµœì  ëª¨ë¸ ë¡œë“œ ë° ì»´íŒŒì¼
    best_model = tf.keras.models.load_model(checkpoint_filepath, compile=False)
    best_model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
        loss={
            'tabular_feature_vector': None,  
            'tabular_classification_output': 'sparse_categorical_crossentropy'
        },
        metrics={'tabular_classification_output': 'accuracy'}
    )
    
    # evaluate() ì‹¤í–‰
    evaluation_results = best_model.evaluate(
        X_test, 
        (None, y_test),
        verbose=0
    )
    
    # ê²°ê³¼ ì¶œë ¥
    metrics_names = best_model.metrics_names
    
    if len(evaluation_results) >= 2:
        total_loss = evaluation_results[0]
        accuracy = evaluation_results[-1] 
        
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ê²°ê³¼:")
        print(f"   Total Loss: {total_loss:.4f}")
        print(f"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
    else:
        print(f"ê²½ê³ : í‰ê°€ ì§€í‘œ(Accuracy)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"Total Loss: {evaluation_results[0]:.4f}")
        print(f"Keras Metrics Names: {metrics_names}")
    
    # ì¶”ê°€ ë¶„ì„: í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ê²°ê³¼
    print("\n--- í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„ì„ ---")
    _, y_pred_probs = best_model.predict(X_test, verbose=0)
    y_pred = np.argmax(y_pred_probs, axis=1)
    
    from sklearn.metrics import classification_report, confusion_matrix
    
    print("\ní˜¼ë™ í–‰ë ¬ (Confusion Matrix):")
    print(confusion_matrix(y_test, y_pred))
    
    print("\në¶„ë¥˜ ë¦¬í¬íŠ¸ (Classification Report):")
    target_names = ['CN (0)', 'MCI (1)', 'Dementia (2)']
    print(classification_report(y_test, y_pred, target_names=target_names))
    
except Exception as e:
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# =========================================================================
# 9. Scaler ì €ì¥
# =========================================================================
import pickle

scaler_filepath = 'dnn_scaler_object_no_scd.pkl'
try:
    with open(scaler_filepath, 'wb') as file:
        pickle.dump(scaler, file)
    print(f"\nâœ… StandardScaler ê°ì²´ê°€ '{scaler_filepath}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"\nâŒ StandardScaler ê°ì²´ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

print("\n" + "=" * 70)
print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print("=" * 70)
print(f"\nìƒì„±ëœ íŒŒì¼:")
print(f"  - {checkpoint_filepath}")
print(f"  - {scaler_filepath}")
print(f"  - dnn_tabular_*_no_scd.csv (4ê°œ íŒŒì¼)")