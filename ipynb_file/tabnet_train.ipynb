{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72EbQnX-XAge",
        "outputId": "5846d484-ac2f-40f9-b1e1-f9d651f85914"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ \n",
        "# ==========================================\n",
        "!pip install pytorch-tabnet imbalanced-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from google.colab import drive\n",
        "\n",
        "# ==========================================\n",
        "# 2. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²° ë° ë°ì´í„° ë¡œë“œ\n",
        "# ==========================================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "base_path = '/content/drive/MyDrive/AiWemeet/data/'\n",
        "print(\"\\n ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...\")\n",
        "\n",
        "try:\n",
        "    # ì—‘ì…€ íŒŒì¼ ë¡œë“œ\n",
        "    df_screen = pd.read_excel(base_path + 'screening_data_1001.xlsx')\n",
        "    df_snsb = pd.read_excel(base_path + 'SNSB_1000.xlsx')\n",
        "    df_apoe = pd.read_excel(base_path + 'APOE_982.xlsx')\n",
        "    df_pet = pd.read_excel(base_path + 'PET_result_300.xlsx')\n",
        "\n",
        "    # MRI ë°ì´í„° (3ê°œ ì‹œíŠ¸ ë¡œë“œ)\n",
        "    mri_path = base_path + 'mri_roi_992.xlsx'\n",
        "    df_mri_sub = pd.read_excel(mri_path, sheet_name='subcortical')\n",
        "    df_mri_surf = pd.read_excel(mri_path, sheet_name='surface')\n",
        "    df_mri_thick = pd.read_excel(mri_path, sheet_name='thickness')\n",
        "    print(\" ëª¨ë“  íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\" [ì—ëŸ¬] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "    raise\n",
        "\n",
        "# ==========================================\n",
        "# 3. ë°ì´í„° í†µí•© \n",
        "# ==========================================\n",
        "# ID ì»¬ëŸ¼ëª… í†µì¼\n",
        "if 'SubjectID' in df_screen.columns:\n",
        "    df_screen = df_screen.rename(columns={'SubjectID': 'ID'})\n",
        "\n",
        "dfs = [df_snsb, df_apoe, df_pet, df_mri_sub, df_mri_surf, df_mri_thick]\n",
        "for df in dfs:\n",
        "    if 'Subject ID' in df.columns:\n",
        "        df.rename(columns={'Subject ID': 'ID'}, inplace=True)\n",
        "    elif 'Subject_ID' in df.columns:\n",
        "        df.rename(columns={'Subject_ID': 'ID'}, inplace=True)\n",
        "\n",
        "# MRI ë³‘í•©\n",
        "drop_cols = ['ìˆœë²ˆ', 'MRI ID', 'NO']\n",
        "df_mri = df_mri_sub.drop(columns=drop_cols, errors='ignore') \\\n",
        "    .merge(df_mri_surf.drop(columns=drop_cols, errors='ignore'), on='ID', how='outer') \\\n",
        "    .merge(df_mri_thick.drop(columns=drop_cols, errors='ignore'), on='ID', how='outer')\n",
        "\n",
        "# ì „ì²´ í†µí•©\n",
        "df_master = df_screen.merge(df_snsb.drop(columns=['ìˆœë²ˆ'], errors='ignore'), on='ID', how='left') \\\n",
        "                     .merge(df_apoe.drop(columns=['NO', 'ìˆœë²ˆ'], errors='ignore'), on='ID', how='left') \\\n",
        "                     .merge(df_mri, on='ID', how='left') \\\n",
        "                     .merge(df_pet.drop(columns=['ìˆœë²ˆ'], errors='ignore'), on='ID', how='left')\n",
        "\n",
        "# ==========================================\n",
        "# 4. ë°ì´í„° ì „ì²˜ë¦¬ (Preprocessing)\n",
        "# ==========================================\n",
        "# Target ë§¤í•‘\n",
        "target_mapping = {'CN': 0, 'SCD': 1, 'MCI': 2, 'Dem': 3}\n",
        "df_master['target'] = df_master['DIA_01'].map(target_mapping)\n",
        "df_master = df_master.dropna(subset=['target'])\n",
        "\n",
        "# ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°\n",
        "cols_to_drop = ['ID', 'SubjectID', 'Num', 'Selection_num', 'test_day', 'Date',\n",
        "                'DIA_01', 'target', 'MRI_ID', 'PET ID', 'ê²€ì‚¬ì¼ì', 'ìˆœë²ˆ']\n",
        "feature_cols = [c for c in df_master.columns if c not in cols_to_drop]\n",
        "\n",
        "# íŒŒìƒë³€ìˆ˜ ìƒì„± (PET ìœ ë¬´, APOE e4)\n",
        "if 'ì•„ë°€ë¡œì´ë“œ ë² íƒ€' in df_master.columns:\n",
        "    df_master['has_PET'] = df_master['ì•„ë°€ë¡œì´ë“œ ë² íƒ€'].notnull().astype(int)\n",
        "    feature_cols.append('has_PET')\n",
        "\n",
        "def check_e4(x):\n",
        "    if pd.isna(x): return 0\n",
        "    return 1 if 'e4' in str(x).lower() else 0\n",
        "if 'APOE' in df_master.columns:\n",
        "    df_master['APOE_e4'] = df_master['APOE'].apply(check_e4)\n",
        "    feature_cols.append('APOE_e4')\n",
        "\n",
        "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
        "df_train = df_master.copy()\n",
        "for col in feature_cols:\n",
        "    if df_train[col].dtype in ['int64', 'float64']:\n",
        "        mean_val = df_train[col].mean()\n",
        "        df_train[col] = df_train[col].fillna(0 if pd.isna(mean_val) else mean_val)\n",
        "    else:\n",
        "        df_train[col] = df_train[col].fillna(\"Unknown\")\n",
        "        le = LabelEncoder()\n",
        "        df_train[col] = le.fit_transform(df_train[col].astype(str))\n",
        "\n",
        "# ì”ì—¬ NaN ì²˜ë¦¬\n",
        "if df_train[feature_cols].isnull().sum().sum() > 0:\n",
        "    df_train[feature_cols] = df_train[feature_cols].fillna(0)\n",
        "\n",
        "X = df_train[feature_cols].values.astype('float32')\n",
        "y = df_train['target'].values.astype(int)\n",
        "\n",
        "# ==========================================\n",
        "# 5. ë°ì´í„° ë¶„í•  ë° SMOTE ì ìš©\n",
        "# ==========================================\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(\"\\n SMOTEë¡œ ë°ì´í„° ë¶ˆê· í˜• í•´ê²° ì¤‘...\")\n",
        "smote = SMOTE(k_neighbors=1, random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test_data ë”°ë¡œ ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1. ì €ì¥í•  ìœ„ì¹˜ (êµ¬ê¸€ ë“œë¼ì´ë¸Œ data í´ë”)\n",
        "save_path = '/content/drive/MyDrive/AiWemeet/data/' \n",
        "\n",
        "# 2. ì €ì¥ ì‹¤í–‰\n",
        "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥\")\n",
        "np.save(save_path + 'X_test_data.npy', X_test)\n",
        "np.save(save_path + 'y_test_data.npy', y_test)\n",
        "\n",
        "print(f\"ì €ì¥ ìœ„ì¹˜: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 6. ì•™ìƒë¸” í•™ìŠµ ì‹œì‘ (5íšŒ ë°˜ë³µ)\n",
        "# ==========================================\n",
        "print(\"\\n ì•™ìƒë¸” í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤! (ì´ 5íšŒ ë°˜ë³µ)\")\n",
        "seeds = [42, 2023, 2024, 777, 999]\n",
        "preds_test_probs = []\n",
        "\n",
        "for i, seed in enumerate(seeds):\n",
        "    print(f\"\\n[Model {i+1}/5] í•™ìŠµ ì¤‘ (Seed={seed})...\")\n",
        "\n",
        "    clf = TabNetClassifier(\n",
        "        n_d=64, n_a=64, n_steps=5,\n",
        "        gamma=1.2,\n",
        "        n_independent=2, n_shared=2,\n",
        "        lambda_sparse=1e-4,\n",
        "        optimizer_fn=torch.optim.Adam,\n",
        "        optimizer_params=dict(lr=1e-2),\n",
        "        scheduler_params={\"step_size\":20, \"gamma\":0.9},\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "        mask_type='entmax',\n",
        "        seed=seed,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_train=X_train_res, y_train=y_train_res,\n",
        "        eval_set=[(X_valid, y_valid)],\n",
        "        eval_name=['valid'],\n",
        "        eval_metric=['balanced_accuracy'],\n",
        "        max_epochs=150,\n",
        "        patience=40,\n",
        "        batch_size=256,\n",
        "        virtual_batch_size=32,\n",
        "        num_workers=0,\n",
        "        drop_last=False,\n",
        "        weights=1\n",
        "    )\n",
        "\n",
        "    pred_prob = clf.predict_proba(X_test)\n",
        "    preds_test_probs.append(pred_prob)\n",
        "    print(f\"  -> Best Valid Score: {clf.best_cost:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 7. ìµœì¢… ê²°ê³¼ ì¢…í•© \n",
        "# ==========================================\n",
        "print(\"\\n ê²°ê³¼ ì¢…í•© ì¤‘...\")\n",
        "avg_preds_proba = np.mean(preds_test_probs, axis=0)\n",
        "final_preds = np.argmax(avg_preds_proba, axis=1)\n",
        "\n",
        "final_f1 = f1_score(y_test, final_preds, average='macro')\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\" ì•™ìƒë¸” ìµœì¢… Macro F1 Score: {final_f1:.4f}\")\n",
        "print(\"=\"*40)\n",
        "print(classification_report(y_test, final_preds, target_names=['CN', 'SCD', 'MCI', 'Dem'], zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpw97fblD-_G",
        "outputId": "bccd6222-5e62-4e53-9fce-36336da8f6d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ”§ SCD(ì£¼ê´€ì  ì¸ì§€ì €í•˜) í´ë˜ìŠ¤ êµ¬ì¶œ ì‘ì „ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
            "\n",
            "âœ¨ ì°¾ì•˜ë‹¤! SCD ìµœì  ì„ê³„ê°’: 0.30\n",
            "   (SCD í™•ë¥ ì´ 30%ë§Œ ë„˜ì–´ë„ SCDë¼ê³  íŒë‹¨í•©ë‹ˆë‹¤)\n",
            "\n",
            "========================================\n",
            "ğŸ† ë³´ì • í›„ ìµœì¢… Macro F1 Score: 0.6022\n",
            "========================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          CN       0.87      0.81      0.84        91\n",
            "         SCD       0.17      0.40      0.24         5\n",
            "         MCI       0.72      0.72      0.72        46\n",
            "         Dem       0.67      0.57      0.62         7\n",
            "\n",
            "    accuracy                           0.76       149\n",
            "   macro avg       0.61      0.63      0.60       149\n",
            "weighted avg       0.79      0.76      0.77       149\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 8. [ì„±ëŠ¥ ê·¹ëŒ€í™”] ì„ê³„ê°’ ìµœì í™”\n",
        "# ==========================================\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "print(\"\\nğŸ”§ SCD(ì£¼ê´€ì  ì¸ì§€ì €í•˜) í´ë˜ìŠ¤ êµ¬ì¶œ ì‘ì „ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "\n",
        "# 5ê°œ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥  í‰ê· ê°’\n",
        "# avg_preds_proba: [CNí™•ë¥ , SCDí™•ë¥ , MCIí™•ë¥ , Demí™•ë¥ ]\n",
        "\n",
        "\n",
        "best_score = 0\n",
        "best_th = 0\n",
        "final_optimized_preds = []\n",
        "\n",
        "# SCD(ì¸ë±ìŠ¤ 1)ì˜ ê¸°ì¤€ì„ 0.05ë¶€í„° 0.5ê¹Œì§€ ë°”ê¿”ê°€ë©° í…ŒìŠ¤íŠ¸\n",
        "for th in np.arange(0.05, 0.55, 0.05):\n",
        "    temp_preds = []\n",
        "    for probs in avg_preds_proba:\n",
        "        # ë§Œì•½ SCDì¼ í™•ë¥ ì´ ì„¤ì •í•œ ê¸°ì¤€(th)ë³´ë‹¤ ë†’ìœ¼ë©´ -> ë¬´ì¡°ê±´ SCD(1)ë¡œ íŒë‹¨\n",
        "        if probs[1] >= th:\n",
        "            temp_preds.append(1) # SCD Class Index = 1\n",
        "        else:\n",
        "            # ì•„ë‹ˆë©´ ì›ë˜ëŒ€ë¡œ í™•ë¥  1ë“±ì¸ í´ë˜ìŠ¤ ì„ íƒ\n",
        "            temp_preds.append(np.argmax(probs))\n",
        "\n",
        "    # ì ìˆ˜ ê³„ì‚°\n",
        "    score = f1_score(y_test, temp_preds, average='macro')\n",
        "\n",
        "    # ì ìˆ˜ê°€ ì˜¤ë¥´ë©´ ê¸°ë¡\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_th = th\n",
        "        final_optimized_preds = temp_preds\n",
        "\n",
        "print(f\"\\n SCD ìµœì  ì„ê³„ê°’: {best_th:.2f}\")\n",
        "print(f\"   (SCD í™•ë¥ ì´ {best_th*100:.0f}%ë§Œ ë„˜ì–´ë„ SCDë¼ê³  íŒë‹¨í•©ë‹ˆë‹¤)\")\n",
        "\n",
        "# ==========================================\n",
        "# 9. ìµœì í™”ëœ ìµœì¢… ì ìˆ˜\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\" ë³´ì • í›„ ìµœì¢… Macro F1 Score: {best_score:.4f}\")\n",
        "print(\"=\"*40)\n",
        "print(classification_report(y_test, final_optimized_preds, target_names=['CN', 'SCD', 'MCI', 'Dem'], zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqBZzF7DHSij",
        "outputId": "818e5831-0306-44d6-939f-df97fc7d351c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ í´ë” ìƒì„± ì™„ë£Œ: /content/drive/MyDrive/AiWemeet/models/\n",
            "ğŸ’¾ ëª¨ë¸ ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
            "\n",
            "Early stopping occurred at epoch 85 with best_epoch = 45 and best_valid_balanced_accuracy = 0.61728\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully saved model at /content/drive/MyDrive/AiWemeet/models/tabnet_model_seed_42.zip\n",
            "  [1/5] ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/AiWemeet/models/tabnet_model_seed_42.zip\n",
            "\n",
            "Early stopping occurred at epoch 86 with best_epoch = 46 and best_valid_balanced_accuracy = 0.65321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully saved model at /content/drive/MyDrive/AiWemeet/models/tabnet_model_seed_2023.zip\n",
            "  [2/5] ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/AiWemeet/models/tabnet_model_seed_2023.zip\n",
            "\n",
            "Early stopping occurred at epoch 70 with best_epoch = 30 and best_valid_balanced_accuracy = 0.57894\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully saved model at /content/drive/MyDrive/AiWemeet/models/tabnet_model_seed_2024.zip\n",
            "  [3/5] ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/AiWemeet/models/tabnet_model_seed_2024.zip\n",
            "\n",
            "Early stopping occurred at epoch 91 with best_epoch = 51 and best_valid_balanced_accuracy = 0.61749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully saved model at /content/drive/MyDrive/AiWemeet/models/tabnet_model_seed_777.zip\n",
            "  [4/5] ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/AiWemeet/models/tabnet_model_seed_777.zip\n",
            "\n",
            "Early stopping occurred at epoch 89 with best_epoch = 49 and best_valid_balanced_accuracy = 0.6033\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully saved model at /content/drive/MyDrive/AiWemeet/models/tabnet_model_seed_999.zip\n",
            "  [5/5] ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/AiWemeet/models/tabnet_model_seed_999.zip\n",
            "\n",
            "âœ¨ ëª¨ë“  ëª¨ë¸ íŒŒì¼ì´ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 1. ì €ì¥í•  í´ë” ë§Œë“¤ê¸°\n",
        "save_path = '/content/drive/MyDrive/AiWemeet/models/'\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "    print(f\" í´ë” ìƒì„± ì™„ë£Œ: {save_path}\")\n",
        "\n",
        "print(\" ëª¨ë¸ ì €ì¥ ì‹œì‘\")\n",
        "\n",
        "# 2. 5ë²ˆ ë°˜ë³µí•˜ë©° ëª¨ë¸ ì¬í•™ìŠµ ë° ì €ì¥\n",
        "seeds = [42, 2023, 2024, 777, 999]\n",
        "\n",
        "for i, seed in enumerate(seeds):\n",
        "    filename = f'tabnet_model_seed_{seed}'\n",
        "\n",
        "    # ëª¨ë¸ ì •ì˜\n",
        "    clf = TabNetClassifier(\n",
        "        n_d=64, n_a=64, n_steps=5, gamma=1.2, n_independent=2, n_shared=2, lambda_sparse=1e-4,\n",
        "        optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=1e-2),\n",
        "        scheduler_params={\"step_size\":20, \"gamma\":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "        mask_type='entmax', seed=seed, verbose=0\n",
        "    )\n",
        "\n",
        "    # í•™ìŠµ\n",
        "    clf.fit(\n",
        "        X_train=X_train_res, y_train=y_train_res,\n",
        "        eval_set=[(X_valid, y_valid)], eval_name=['valid'], eval_metric=['balanced_accuracy'],\n",
        "        max_epochs=150, patience=40, batch_size=256, virtual_batch_size=32,\n",
        "        num_workers=0, drop_last=False, weights=1\n",
        "    )\n",
        "\n",
        "    # ì €ì¥\n",
        "    saved_filepath = clf.save_model(save_path + filename)\n",
        "    print(f\"  [{i+1}/5] ì €ì¥ ì™„ë£Œ: {saved_filepath}\")\n",
        "\n",
        "print(\"\\n ëª¨ë“  ëª¨ë¸ íŒŒì¼ì´ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì €ì¥ë¨\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
